@article{BanfRaft-1993,
  author =	 {Jeffrey D. Banfield and Adrian E. Raftery},
  year =	 1993,
  title =	 {Model-Based Gaussian and Non-Gaussian Clustering},
  journal =	 {Biometrics},
  volume =	 49,
  number =	 3,
  pages =	 {803--821},
  publisher =	 {[Wiley, International Biometric Society]},
  ISSN =	 {0006341X, 15410420},
  DOI =		 {10.2307/2532201},
  URL =		 {http://www.jstor.org/stable/2532201},
  abstract =	 {The classification maximum likelihood approach is
                  sufficiently general to encompass many current clustering
                  algorithms, including those based on the sum of squares
                  criterion and on the criterion of Friedman and Rubin
                  (1967, Journal of the American Statistical Association
                  62, 1159-1178). However, as currently implemented, it
                  does not allow the specification of which features
                  (orientation, size, and shape) are to be common to all
                  clusters and which may differ between clusters. Also, it
                  is restricted to Gaussian distributions and it does not
                  allow for noise. We propose ways of overcoming these
                  limitations. A reparameterization of the covariance
                  matrix allows us to specify that some, but not all,
                  features be the same for all clusters. A practical
                  framework for non-Gaussian clustering is outlined, and a
                  means of incorporating noise in the form of a Poisson
                  process is described. An approximate Bayesian method for
                  choosing the number of clusters is given. The performance
                  of the proposed methods is studied by simulation, with
                  encouraging results. The methods are applied to the
                  analysis of a data set arising in the study of diabetes,
                  and the results seem better than those of previous
                  analyses. A magnetic resonance image (MRI) of the brain
                  is also analyzed, and the methods appear successful in
                  extracting the main features of anatomical interest. The
                  methods described here have been implemented in both
                  Fortran and S-PLUS versions, and the software is freely
                  available through StatLib.},
}

@article{CeleuxGov-1995,
 author =	 "Gilles Celeux and Gérard Govaert",
  year  =	 1995,
  title =	 "Gaussian parsimonious clustering models",
  journal =	 "Pattern Recognition",
  volume =	 28,
  number =	 5,
  pages =	 {781--793},
  issn =	 "0031-3203",
  doi =		 {10.1016/0031-3203(94)00125-6},
  url = {http://www.sciencedirect.com/science/article/pii/0031320394001256},
  keywords =	 "Gaussian mixture, Eigenvalue decomposition, Cluster volumes",
  abstract =	 "Gaussian clustering models are useful both for
                  understanding and suggesting powerful criteria. Banfield
                  and Raftery, Biometriks 49, 803–821 (1993), have
                  considered a parameterization of the variance matrix Σk
                  of a cluster Pk in terms of its eigenvalue decomposition,
                  Σk = λkDkAkDk′ where λk defines the volume of Pk, Dk is
                  an orthogonal matrix which defines its orientation and Ak
                  is a diagonal matrix with determinant 1 which defines its
                  shape. This parametrization allows us to propose many
                  general clustering criteria from the simplest one
                  (spherical clusters with equal volumes which leads to the
                  classical k-means criterion) to the most complex one
                  (unknown and different volumes, orientations and shapes
                  for all clusters). Methods of optimization to derive the
                  maximum likelihood estimates as well as the practical
                  usefulness of these models are discussed. We especially
                  analyse the influence of the volumes of clusters. We
                  report Monte Carlo simulations and an application on
                  stellar data which dramatically illustrated the relevance
                  of allowing clusters to have different volumes."
}

@article{AndrewsMS-2011,
  author = {Jeffrey L. Andrews and Paul D. McNicholas and Sanjeena Subedi},
  year =   2011,
  title = {Model-based classification via mixtures of multivariate
                  t-distributions} ,
  journal = {Computational Statistics & Data Analysis},
  volume =	 55,
  number =	 1,
  pages =	 "520 - 529",
  issn =	 "0167-9473",
  doi =	{10.1016/j.csda.2010.05.019},
  url = {http://www.sciencedirect.com/science/article/pii/S0167947310002203},
  keywords =	 "Classification, Food authenticity, Mixture models,
                  Model-based classification, Multivariate -distributions",
  abstract =	 "A novel model-based classification technique is
                  introduced based on mixtures of multivariate
                  t-distributions. A family of four mixture models is
                  defined by constraining, or not, the covariance matrices
                  and the degrees of freedom to be equal across mixture
                  components. Parameters for each of the resulting four
                  models are estimated using a multicycle
                  expectation–conditional maximization algorithm, where
                  convergence is determined using a criterion based on the
                  Aitken acceleration. A straightforward, but very
                  effective, technique for the initialization of the
                  unknown component memberships is introduced and compared
                  with a popular, more sophisticated, initialization
                  procedure. This novel four-member family is applied to
                  real and simulated data, where it gives good
                  classification performance, even when compared with more
                  established techniques."
}

@article{BouvBrun-2014,
  author =	 "Charles Bouveyron and Camille Brunet-Saumard",
  year =	 2014,
  title =	 "Model-based clustering of high-dimensional data: A
                  review",
  journal =	 "Computational Statistics & Data Analysis",
  volume =	 71,
  pages =	 {52--78},
  issn =	 "0167-9473",
  doi =		 {10.1016/j.csda.2012.12.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0167947312004422},
  keywords =	 "Model-based clustering, High-dimensional data, Dimension
                  reduction, Regularization, Parsimonious models, Subspace
                  clustering, Variable selection, Software, R package",
  abstract =	 "Model-based clustering is a popular tool which is
                  renowned for its probabilistic foundations and its
                  flexibility. However, high-dimensional data are nowadays
                  more and more frequent and, unfortunately, classical
                  model-based clustering techniques show a disappointing
                  behavior in high-dimensional spaces. This is mainly due
                  to the fact that model-based clustering methods are
                  dramatically over-parametrized in this case. However,
                  high-dimensional spaces have specific characteristics
                  which are useful for clustering and recent techniques
                  exploit those characteristics. After having recalled the
                  bases of model-based clustering, dimension reduction
                  approaches, regularization-based techniques, parsimonious
                  modeling, subspace clustering methods and clustering
                  methods based on variable selection are
                  reviewed. Existing softwares for model-based clustering
                  of high-dimensional data will be also reviewed and their
                  practical use will be illustrated on real-world data
                  sets."
}
